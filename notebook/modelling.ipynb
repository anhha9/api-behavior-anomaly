{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "There are 2 aims of this notebook:\n",
    "- Develop a supervised-learning model to classify the API call behaviours into normal and abnormal behaviors\n",
    "- Evaluate the developed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_id</th>\n",
       "      <th>inter_api_access_duration(sec)</th>\n",
       "      <th>api_access_uniqueness</th>\n",
       "      <th>sequence_length(count)</th>\n",
       "      <th>vsession_duration(min)</th>\n",
       "      <th>ip_type</th>\n",
       "      <th>num_sessions</th>\n",
       "      <th>num_users</th>\n",
       "      <th>num_unique_apis</th>\n",
       "      <th>source</th>\n",
       "      <th>classification</th>\n",
       "      <th>is_anomaly</th>\n",
       "      <th>mean_page_rank</th>\n",
       "      <th>min_in_degree</th>\n",
       "      <th>std_out_degree</th>\n",
       "      <th>min_clustering_coefficient</th>\n",
       "      <th>std_page_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1f2c32d8-2d6e-3b68-bc46-789469f2b71e</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>85.643243</td>\n",
       "      <td>5405</td>\n",
       "      <td>default</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>1295.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>E</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0</td>\n",
       "      <td>12.437627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4c486414-d4f5-33f6-b485-24a8ed2925e8</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>16.166805</td>\n",
       "      <td>519</td>\n",
       "      <td>default</td>\n",
       "      <td>9299.0</td>\n",
       "      <td>8447.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>E</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0</td>\n",
       "      <td>8.032560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7e5838fc-bce1-371f-a3ac-d8a0b2a05d9a</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>99.573276</td>\n",
       "      <td>6211</td>\n",
       "      <td>default</td>\n",
       "      <td>255.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>E</td>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0</td>\n",
       "      <td>8.253844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                   _id  \\\n",
       "0           0  1f2c32d8-2d6e-3b68-bc46-789469f2b71e   \n",
       "1           1  4c486414-d4f5-33f6-b485-24a8ed2925e8   \n",
       "2           2  7e5838fc-bce1-371f-a3ac-d8a0b2a05d9a   \n",
       "\n",
       "   inter_api_access_duration(sec)  api_access_uniqueness  \\\n",
       "0                        0.000812               0.004066   \n",
       "1                        0.000063               0.002211   \n",
       "2                        0.004481               0.015324   \n",
       "\n",
       "   sequence_length(count)  vsession_duration(min)  ip_type  num_sessions  \\\n",
       "0               85.643243                    5405  default        1460.0   \n",
       "1               16.166805                     519  default        9299.0   \n",
       "2               99.573276                    6211  default         255.0   \n",
       "\n",
       "   num_users  num_unique_apis source classification  is_anomaly  \\\n",
       "0     1295.0            451.0      E         normal       False   \n",
       "1     8447.0            302.0      E         normal       False   \n",
       "2      232.0            354.0      E         normal       False   \n",
       "\n",
       "   mean_page_rank  min_in_degree  std_out_degree  min_clustering_coefficient  \\\n",
       "0        0.002237              0       12.437627                         0.0   \n",
       "1        0.003571              0        8.032560                         0.0   \n",
       "2        0.002825              0        8.253844                         0.0   \n",
       "\n",
       "   std_page_rank  \n",
       "0       0.004533  \n",
       "1       0.006145  \n",
       "2       0.004735  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = pd.read_csv('/mnt/c/Users/haanh/api-behavior-anomaly/data/all_features.csv')\n",
    "\n",
    "all_features.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "- Since 'Unnamed' column is just to indicate the original indices of each _id and it does not have any predictive value, it will be dropped.\n",
    "- Since the 'Classification' column has been transformed into \"is_anomaly\" column where normal equates to FALSE and outlier equates to TRUE. The 'Classification' column will be dropped as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>inter_api_access_duration(sec)</th>\n",
       "      <th>api_access_uniqueness</th>\n",
       "      <th>sequence_length(count)</th>\n",
       "      <th>vsession_duration(min)</th>\n",
       "      <th>ip_type</th>\n",
       "      <th>num_sessions</th>\n",
       "      <th>num_users</th>\n",
       "      <th>num_unique_apis</th>\n",
       "      <th>source</th>\n",
       "      <th>is_anomaly</th>\n",
       "      <th>mean_page_rank</th>\n",
       "      <th>min_in_degree</th>\n",
       "      <th>std_out_degree</th>\n",
       "      <th>min_clustering_coefficient</th>\n",
       "      <th>std_page_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1f2c32d8-2d6e-3b68-bc46-789469f2b71e</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>85.643243</td>\n",
       "      <td>5405</td>\n",
       "      <td>default</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>1295.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>E</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0</td>\n",
       "      <td>12.437627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4c486414-d4f5-33f6-b485-24a8ed2925e8</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>16.166805</td>\n",
       "      <td>519</td>\n",
       "      <td>default</td>\n",
       "      <td>9299.0</td>\n",
       "      <td>8447.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>E</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0</td>\n",
       "      <td>8.032560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7e5838fc-bce1-371f-a3ac-d8a0b2a05d9a</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>99.573276</td>\n",
       "      <td>6211</td>\n",
       "      <td>default</td>\n",
       "      <td>255.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>E</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0</td>\n",
       "      <td>8.253844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    _id  inter_api_access_duration(sec)  \\\n",
       "0  1f2c32d8-2d6e-3b68-bc46-789469f2b71e                        0.000812   \n",
       "1  4c486414-d4f5-33f6-b485-24a8ed2925e8                        0.000063   \n",
       "2  7e5838fc-bce1-371f-a3ac-d8a0b2a05d9a                        0.004481   \n",
       "\n",
       "   api_access_uniqueness  sequence_length(count)  vsession_duration(min)  \\\n",
       "0               0.004066               85.643243                    5405   \n",
       "1               0.002211               16.166805                     519   \n",
       "2               0.015324               99.573276                    6211   \n",
       "\n",
       "   ip_type  num_sessions  num_users  num_unique_apis source  is_anomaly  \\\n",
       "0  default        1460.0     1295.0            451.0      E       False   \n",
       "1  default        9299.0     8447.0            302.0      E       False   \n",
       "2  default         255.0      232.0            354.0      E       False   \n",
       "\n",
       "   mean_page_rank  min_in_degree  std_out_degree  min_clustering_coefficient  \\\n",
       "0        0.002237              0       12.437627                         0.0   \n",
       "1        0.003571              0        8.032560                         0.0   \n",
       "2        0.002825              0        8.253844                         0.0   \n",
       "\n",
       "   std_page_rank  \n",
       "0       0.004533  \n",
       "1       0.006145  \n",
       "2       0.004735  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = all_features.drop(['Unnamed: 0', 'classification'], axis=1)\n",
    "\n",
    "all_features.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 'min_in_degree' and 'min_clustering_coefficient' all contain 0. In other words, these columns do not provide good information. Hence, they will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>inter_api_access_duration(sec)</th>\n",
       "      <th>api_access_uniqueness</th>\n",
       "      <th>sequence_length(count)</th>\n",
       "      <th>vsession_duration(min)</th>\n",
       "      <th>ip_type</th>\n",
       "      <th>num_sessions</th>\n",
       "      <th>num_users</th>\n",
       "      <th>num_unique_apis</th>\n",
       "      <th>source</th>\n",
       "      <th>is_anomaly</th>\n",
       "      <th>mean_page_rank</th>\n",
       "      <th>std_out_degree</th>\n",
       "      <th>std_page_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1f2c32d8-2d6e-3b68-bc46-789469f2b71e</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>85.643243</td>\n",
       "      <td>5405</td>\n",
       "      <td>default</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>1295.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>E</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>12.437627</td>\n",
       "      <td>0.004533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4c486414-d4f5-33f6-b485-24a8ed2925e8</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>16.166805</td>\n",
       "      <td>519</td>\n",
       "      <td>default</td>\n",
       "      <td>9299.0</td>\n",
       "      <td>8447.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>E</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>8.032560</td>\n",
       "      <td>0.006145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7e5838fc-bce1-371f-a3ac-d8a0b2a05d9a</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>99.573276</td>\n",
       "      <td>6211</td>\n",
       "      <td>default</td>\n",
       "      <td>255.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>E</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>8.253844</td>\n",
       "      <td>0.004735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    _id  inter_api_access_duration(sec)  \\\n",
       "0  1f2c32d8-2d6e-3b68-bc46-789469f2b71e                        0.000812   \n",
       "1  4c486414-d4f5-33f6-b485-24a8ed2925e8                        0.000063   \n",
       "2  7e5838fc-bce1-371f-a3ac-d8a0b2a05d9a                        0.004481   \n",
       "\n",
       "   api_access_uniqueness  sequence_length(count)  vsession_duration(min)  \\\n",
       "0               0.004066               85.643243                    5405   \n",
       "1               0.002211               16.166805                     519   \n",
       "2               0.015324               99.573276                    6211   \n",
       "\n",
       "   ip_type  num_sessions  num_users  num_unique_apis source  is_anomaly  \\\n",
       "0  default        1460.0     1295.0            451.0      E       False   \n",
       "1  default        9299.0     8447.0            302.0      E       False   \n",
       "2  default         255.0      232.0            354.0      E       False   \n",
       "\n",
       "   mean_page_rank  std_out_degree  std_page_rank  \n",
       "0        0.002237       12.437627       0.004533  \n",
       "1        0.003571        8.032560       0.006145  \n",
       "2        0.002825        8.253844       0.004735  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = all_features.drop(['min_in_degree', 'min_clustering_coefficient'], axis=1)\n",
    "\n",
    "all_features.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>inter_api_access_duration(sec)</th>\n",
       "      <th>api_access_uniqueness</th>\n",
       "      <th>sequence_length(count)</th>\n",
       "      <th>vsession_duration(min)</th>\n",
       "      <th>ip_type</th>\n",
       "      <th>num_sessions</th>\n",
       "      <th>num_users</th>\n",
       "      <th>num_unique_apis</th>\n",
       "      <th>source</th>\n",
       "      <th>is_anomaly</th>\n",
       "      <th>mean_page_rank</th>\n",
       "      <th>std_out_degree</th>\n",
       "      <th>std_page_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1f2c32d8-2d6e-3b68-bc46-789469f2b71e</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>85.643243</td>\n",
       "      <td>5405</td>\n",
       "      <td>1</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>1295.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>12.437627</td>\n",
       "      <td>0.004533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4c486414-d4f5-33f6-b485-24a8ed2925e8</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>16.166805</td>\n",
       "      <td>519</td>\n",
       "      <td>1</td>\n",
       "      <td>9299.0</td>\n",
       "      <td>8447.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>8.032560</td>\n",
       "      <td>0.006145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7e5838fc-bce1-371f-a3ac-d8a0b2a05d9a</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>99.573276</td>\n",
       "      <td>6211</td>\n",
       "      <td>1</td>\n",
       "      <td>255.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>8.253844</td>\n",
       "      <td>0.004735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    _id  inter_api_access_duration(sec)  \\\n",
       "0  1f2c32d8-2d6e-3b68-bc46-789469f2b71e                        0.000812   \n",
       "1  4c486414-d4f5-33f6-b485-24a8ed2925e8                        0.000063   \n",
       "2  7e5838fc-bce1-371f-a3ac-d8a0b2a05d9a                        0.004481   \n",
       "\n",
       "   api_access_uniqueness  sequence_length(count)  vsession_duration(min)  \\\n",
       "0               0.004066               85.643243                    5405   \n",
       "1               0.002211               16.166805                     519   \n",
       "2               0.015324               99.573276                    6211   \n",
       "\n",
       "   ip_type  num_sessions  num_users  num_unique_apis  source  is_anomaly  \\\n",
       "0        1        1460.0     1295.0            451.0       1       False   \n",
       "1        1        9299.0     8447.0            302.0       1       False   \n",
       "2        1         255.0      232.0            354.0       1       False   \n",
       "\n",
       "   mean_page_rank  std_out_degree  std_page_rank  \n",
       "0        0.002237       12.437627       0.004533  \n",
       "1        0.003571        8.032560       0.006145  \n",
       "2        0.002825        8.253844       0.004735  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding the categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lab = LabelEncoder()\n",
    "\n",
    "# Transform the categorical columns\n",
    "all_features['ip_type'] = lab.fit_transform(all_features['ip_type'])\n",
    "all_features['source'] = lab.fit_transform(all_features['ip_type'])\n",
    "\n",
    "# Display current data\n",
    "all_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define featuees and target\n",
    "x = all_features[['inter_api_access_duration(sec)', 'api_access_uniqueness', 'sequence_length(count)', 'vsession_duration(min)', 'ip_type', 'num_sessions', 'source', 'mean_page_rank', 'std_out_degree', 'std_page_rank']]\n",
    "y = all_features['is_anomaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (1355, 10)\n",
      "Test shape:: (339, 10)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data for training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train shape: \", x_train.shape)\n",
    "print(\"Test shape::\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "Before creating a model, it is important to determine if the problem can be addressed without the use of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010752688172043012, 0.010752688172043012, 0.010752688172043012, 0.0106951871657754, 0.0106951871657754, 0.0106951871657754, 0.0106951871657754, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010638297872340425, 0.010582010582010581, 0.010582010582010581, 0.010582010582010581, 0.010582010582010581, 0.010582010582010581, 0.010582010582010581, 0.010526315789473684, 0.010526315789473684, 0.010526315789473684, 0.010526315789473684, 0.010416666666666666, 0.010416666666666666, 0.010416666666666666, 0.010416666666666666, 0.010416666666666666, 0.010416666666666666, 0.010416666666666666, 0.010416666666666666, 0.010362694300518135, 0.010309278350515464, 0.010256410256410256, 0.010256410256410256, 0.010256410256410256, 0.010256410256410256, 0.010256410256410256, 0.010256410256410256, 0.010256410256410256, 0.010256410256410256, 0.01020408163265306, 0.01020408163265306, 0.01020408163265306, 0.010101010101010102, 0.010101010101010102, 0.010101010101010102, 0.010101010101010102, 0.010101010101010102, 0.010050251256281407, 0.010050251256281407, 0.010050251256281407, 0.010050251256281407, 0.010050251256281407, 0.010050251256281407, 0.01, 0.01, 0.01, 0.01, 0.009950248756218905, 0.009950248756218905, 0.009950248756218905, 0.009900990099009901, 0.009900990099009901, 0.00980392156862745, 0.00980392156862745, 0.00980392156862745, 0.00980392156862745, 0.00980392156862745, 0.00980392156862745, 0.00980392156862745, 0.00975609756097561, 0.00975609756097561, 0.00975609756097561, 0.00975609756097561, 0.009615384615384616, 0.009523809523809525, 0.018957345971563982, 0.018957345971563982, 0.018957345971563982, 0.018867924528301886, 0.018867924528301886, 0.018779342723004695, 0.018779342723004695, 0.018779342723004695, 0.018779342723004695, 0.018691588785046728, 0.018691588785046728, 0.018604651162790697, 0.027777777777777776, 0.027777777777777776, 0.027777777777777776, 0.027649769585253458, 0.027649769585253458, 0.027522935779816515, 0.027522935779816515, 0.027522935779816515, 0.027522935779816515, 0.027522935779816515, 0.02727272727272727, 0.02727272727272727, 0.02727272727272727, 0.02727272727272727, 0.02727272727272727, 0.02727272727272727, 0.02727272727272727, 0.02727272727272727, 0.02727272727272727, 0.02727272727272727, 0.02727272727272727, 0.027149321266968326, 0.027149321266968326, 0.02702702702702703, 0.02702702702702703, 0.026905829596412557, 0.026905829596412557, 0.026905829596412557, 0.026905829596412557, 0.02666666666666667, 0.02666666666666667, 0.02654867256637168, 0.02654867256637168, 0.02654867256637168, 0.02643171806167401, 0.026200873362445413, 0.026200873362445413, 0.02608695652173913, 0.02608695652173913, 0.02608695652173913, 0.02608695652173913, 0.02608695652173913, 0.02608695652173913, 0.02608695652173913, 0.02586206896551724, 0.02586206896551724, 0.02586206896551724, 0.02586206896551724, 0.02586206896551724, 0.02586206896551724, 0.02586206896551724, 0.02586206896551724, 0.02586206896551724, 0.02575107296137339, 0.02575107296137339, 0.02575107296137339, 0.02575107296137339, 0.02575107296137339, 0.02575107296137339, 0.02575107296137339, 0.02575107296137339, 0.02564102564102564, 0.02564102564102564, 0.02564102564102564, 0.02553191489361702, 0.02553191489361702, 0.02553191489361702, 0.025423728813559324, 0.025423728813559324, 0.025210084033613446, 0.025210084033613446, 0.03347280334728033, 0.03347280334728033, 0.03347280334728033, 0.03347280334728033, 0.03333333333333333, 0.03333333333333333, 0.03319502074688797, 0.03319502074688797, 0.03319502074688797, 0.03319502074688797, 0.03319502074688797, 0.03305785123966942, 0.03305785123966942, 0.03305785123966942, 0.03305785123966942, 0.03305785123966942, 0.03305785123966942, 0.03305785123966942, 0.03305785123966942, 0.03305785123966942, 0.03292181069958848, 0.03292181069958848, 0.03292181069958848, 0.03292181069958848, 0.03292181069958848, 0.03278688524590164, 0.03278688524590164, 0.03278688524590164, 0.03278688524590164, 0.03278688524590164, 0.03278688524590164, 0.03278688524590164, 0.03278688524590164, 0.03278688524590164, 0.03278688524590164, 0.03278688524590164, 0.03278688524590164, 0.03278688524590164, 0.0326530612244898, 0.0326530612244898, 0.0326530612244898, 0.0326530612244898, 0.0326530612244898, 0.0326530612244898, 0.0326530612244898, 0.0326530612244898, 0.0326530612244898, 0.0326530612244898, 0.032520325203252036, 0.032520325203252036, 0.032520325203252036, 0.032520325203252036, 0.032520325203252036, 0.032520325203252036, 0.032520325203252036, 0.032520325203252036, 0.032388663967611336, 0.032388663967611336, 0.032388663967611336, 0.032388663967611336, 0.032388663967611336, 0.04032258064516129, 0.04032258064516129, 0.04032258064516129, 0.04032258064516129, 0.04032258064516129, 0.04032258064516129, 0.04032258064516129, 0.04032258064516129, 0.04032258064516129, 0.040160642570281124, 0.040160642570281124, 0.040160642570281124, 0.040160642570281124, 0.040160642570281124, 0.040160642570281124, 0.040160642570281124, 0.040160642570281124, 0.040160642570281124, 0.040160642570281124, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.0398406374501992, 0.03968253968253968, 0.03968253968253968, 0.03968253968253968, 0.03968253968253968, 0.03968253968253968, 0.03968253968253968, 0.03968253968253968, 0.03968253968253968, 0.03968253968253968, 0.03968253968253968, 0.039525691699604744, 0.039525691699604744, 0.039525691699604744, 0.03937007874015748, 0.03937007874015748, 0.03937007874015748, 0.03937007874015748, 0.03937007874015748, 0.03937007874015748, 0.03937007874015748, 0.0392156862745098, 0.0390625, 0.0390625, 0.0390625, 0.0390625, 0.04669260700389105, 0.046511627906976744, 0.046511627906976744, 0.046511627906976744, 0.046511627906976744, 0.04633204633204633, 0.04633204633204633, 0.046153846153846156, 0.046153846153846156, 0.046153846153846156, 0.046153846153846156, 0.046153846153846156, 0.046153846153846156, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.04580152671755725, 0.045283018867924525, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05947955390334572, 0.05947955390334572, 0.05947955390334572, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.07380073800738007, 0.08088235294117647, 0.08058608058608059, 0.08058608058608059, 0.08058608058608059, 0.08029197080291971, 0.08029197080291971, 0.08029197080291971, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.07971014492753623, 0.07971014492753623, 0.07971014492753623, 0.07971014492753623, 0.07971014492753623, 0.07971014492753623, 0.07971014492753623, 0.07971014492753623, 0.07971014492753623, 0.07971014492753623, 0.08602150537634409, 0.09285714285714286, 0.09252669039145907, 0.09252669039145907, 0.09252669039145907, 0.09252669039145907, 0.09929078014184398, 0.09929078014184398, 0.09929078014184398, 0.09929078014184398, 0.09929078014184398, 0.09929078014184398, 0.09929078014184398, 0.09929078014184398, 0.09929078014184398, 0.09929078014184398, 0.09929078014184398, 0.0989399293286219, 0.1056338028169014, 0.1056338028169014, 0.1048951048951049, 0.10416666666666667, 0.10416666666666667, 0.10416666666666667, 0.10380622837370242, 0.10380622837370242, 0.10380622837370242, 0.10380622837370242, 0.10380622837370242, 0.10380622837370242, 0.10380622837370242, 0.10380622837370242, 0.10380622837370242, 0.10380622837370242, 0.10344827586206896, 0.10344827586206896, 0.10344827586206896, 0.10344827586206896, 0.10344827586206896, 0.10344827586206896, 0.10344827586206896, 0.10344827586206896, 0.10996563573883161, 0.10996563573883161, 0.10996563573883161, 0.10996563573883161, 0.11643835616438356, 0.11643835616438356, 0.11643835616438356, 0.11604095563139932, 0.11604095563139932, 0.1152542372881356, 0.11486486486486487, 0.11486486486486487, 0.11447811447811448, 0.11447811447811448, 0.11447811447811448, 0.11447811447811448, 0.11447811447811448, 0.11447811447811448, 0.12080536912751678, 0.12080536912751678, 0.12080536912751678, 0.12040133779264214, 0.12666666666666668, 0.13245033112582782, 0.132013201320132, 0.132013201320132, 0.132013201320132, 0.132013201320132, 0.132013201320132, 0.132013201320132, 0.132013201320132, 0.132013201320132, 0.132013201320132, 0.13157894736842105, 0.13157894736842105, 0.1377049180327869, 0.1437908496732026, 0.1437908496732026, 0.1437908496732026, 0.1437908496732026, 0.1437908496732026, 0.1437908496732026, 0.1437908496732026, 0.1437908496732026, 0.1437908496732026, 0.1437908496732026, 0.15584415584415584, 0.15584415584415584, 0.15584415584415584, 0.15584415584415584, 0.15584415584415584, 0.15584415584415584, 0.16129032258064516, 0.16129032258064516, 0.16129032258064516, 0.16720257234726688, 0.16720257234726688, 0.16720257234726688, 0.17307692307692307, 0.17307692307692307, 0.17307692307692307, 0.17307692307692307, 0.17252396166134185, 0.17252396166134185, 0.17252396166134185, 0.17252396166134185, 0.17252396166134185, 0.17252396166134185, 0.17252396166134185, 0.17252396166134185, 0.17197452229299362, 0.18354430379746836, 0.1892744479495268, 0.1892744479495268, 0.1892744479495268, 0.1892744479495268, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18867924528301888, 0.18808777429467086, 0.18808777429467086, 0.18808777429467086, 0.18808777429467086, 0.18808777429467086, 0.18808777429467086, 0.1875, 0.1875, 0.1875, 0.1875, 0.18691588785046728, 0.18691588785046728, 0.18575851393188855, 0.18575851393188855, 0.18575851393188855, 0.19076923076923077, 0.1901840490797546, 0.1901840490797546, 0.1901840490797546, 0.1901840490797546, 0.1901840490797546, 0.1901840490797546, 0.20121951219512196, 0.20121951219512196, 0.20121951219512196, 0.20121951219512196, 0.20121951219512196, 0.20121951219512196, 0.2006079027355623, 0.2006079027355623, 0.2006079027355623, 0.2006079027355623, 0.2, 0.2, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.2042042042042042, 0.20359281437125748, 0.20359281437125748, 0.20298507462686566, 0.20238095238095238, 0.20238095238095238, 0.20238095238095238, 0.20178041543026706, 0.20178041543026706, 0.20178041543026706, 0.20178041543026706, 0.20178041543026706, 0.20178041543026706, 0.20118343195266272, 0.20648967551622419, 0.20648967551622419, 0.20648967551622419, 0.20648967551622419, 0.20648967551622419, 0.20648967551622419, 0.20648967551622419, 0.20648967551622419, 0.20648967551622419, 0.20588235294117646, 0.20527859237536658, 0.20527859237536658, 0.2046783625730994, 0.2046783625730994, 0.2046783625730994, 0.20348837209302326, 0.20348837209302326, 0.20348837209302326, 0.20348837209302326, 0.20348837209302326, 0.20348837209302326, 0.2028985507246377, 0.2028985507246377, 0.20809248554913296, 0.20809248554913296, 0.20809248554913296, 0.20809248554913296, 0.20809248554913296, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.207492795389049, 0.21203438395415472, 0.21203438395415472, 0.21142857142857144, 0.21142857142857144, 0.21142857142857144, 0.21142857142857144, 0.21142857142857144, 0.21142857142857144, 0.21142857142857144, 0.21142857142857144, 0.21142857142857144, 0.21142857142857144, 0.21652421652421652, 0.2159090909090909, 0.2159090909090909, 0.2159090909090909, 0.2159090909090909, 0.21529745042492918, 0.21910112359550563, 0.2184873949579832, 0.22346368715083798, 0.22346368715083798, 0.22346368715083798, 0.22346368715083798, 0.22346368715083798, 0.22346368715083798, 0.22284122562674094, 0.22284122562674094, 0.22284122562674094, 0.22160664819944598, 0.22099447513812154, 0.22099447513812154, 0.22099447513812154, 0.22099447513812154, 0.22099447513812154, 0.22099447513812154, 0.22099447513812154, 0.22099447513812154, 0.21978021978021978, 0.21978021978021978, 0.21978021978021978, 0.21978021978021978, 0.21978021978021978, 0.21978021978021978, 0.21978021978021978, 0.21978021978021978, 0.21978021978021978, 0.21978021978021978, 0.21978021978021978, 0.21978021978021978, 0.21978021978021978, 0.21978021978021978, 0.2191780821917808, 0.2191780821917808, 0.2191780821917808, 0.2185792349726776, 0.21798365122615804, 0.21798365122615804, 0.21798365122615804, 0.21798365122615804, 0.21798365122615804, 0.21798365122615804, 0.21798365122615804, 0.21798365122615804, 0.21798365122615804, 0.21798365122615804, 0.21798365122615804, 0.21798365122615804, 0.21739130434782608, 0.21739130434782608, 0.22162162162162163, 0.22162162162162163, 0.22162162162162163, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.2210242587601078, 0.22043010752688172, 0.22043010752688172, 0.22043010752688172, 0.22043010752688172, 0.22459893048128343, 0.22459893048128343, 0.22459893048128343, 0.22459893048128343, 0.22459893048128343, 0.224, 0.224, 0.224, 0.22872340425531915, 0.22872340425531915, 0.22872340425531915, 0.22872340425531915, 0.22872340425531915, 0.22872340425531915, 0.22872340425531915, 0.22872340425531915, 0.22872340425531915, 0.22872340425531915, 0.22872340425531915, 0.23342175066312998, 0.23342175066312998, 0.23342175066312998, 0.23342175066312998, 0.23342175066312998, 0.23342175066312998, 0.23342175066312998, 0.23342175066312998, 0.2328042328042328, 0.23746701846965698, 0.23746701846965698, 0.23684210526315788, 0.23684210526315788, 0.23684210526315788, 0.23684210526315788, 0.23684210526315788, 0.23684210526315788, 0.23684210526315788, 0.23684210526315788, 0.23684210526315788, 0.23684210526315788, 0.24146981627296588, 0.24146981627296588, 0.24607329842931938, 0.24607329842931938, 0.24607329842931938, 0.24607329842931938, 0.2506527415143603, 0.2552083333333333, 0.2597402597402597, 0.2597402597402597, 0.2597402597402597, 0.2597402597402597, 0.2597402597402597, 0.2597402597402597, 0.26424870466321243, 0.26424870466321243, 0.268733850129199, 0.268733850129199, 0.268733850129199, 0.268733850129199, 0.268733850129199, 0.268733850129199, 0.2776349614395887, 0.28205128205128205, 0.28205128205128205, 0.28205128205128205, 0.28205128205128205, 0.28205128205128205, 0.2864450127877238, 0.2864450127877238, 0.2864450127877238, 0.2864450127877238, 0.2864450127877238, 0.2864450127877238, 0.2864450127877238, 0.29081632653061223, 0.2951653944020356, 0.2951653944020356, 0.2951653944020356, 0.2951653944020356, 0.2951653944020356, 0.2951653944020356, 0.2951653944020356, 0.29949238578680204, 0.3037974683544304, 0.30808080808080807, 0.3165829145728643, 0.3165829145728643, 0.3165829145728643, 0.3165829145728643, 0.325, 0.325, 0.32917705735660846, 0.337468982630273, 0.337468982630273, 0.337468982630273, 0.337468982630273, 0.3415841584158416, 0.345679012345679, 0.345679012345679, 0.345679012345679, 0.3538083538083538, 0.3538083538083538, 0.3538083538083538, 0.35784313725490197, 0.35784313725490197, 0.35784313725490197, 0.35784313725490197, 0.35784313725490197, 0.35784313725490197, 0.35784313725490197, 0.36585365853658536, 0.36585365853658536, 0.36585365853658536, 0.36585365853658536, 0.36893203883495146, 0.36893203883495146, 0.36893203883495146, 0.36803874092009686, 0.36803874092009686, 0.3710843373493976, 0.3710843373493976, 0.37410071942446044, 0.37410071942446044, 0.37410071942446044, 0.37410071942446044, 0.37410071942446044, 0.37410071942446044, 0.37410071942446044, 0.37410071942446044, 0.37799043062200954, 0.37799043062200954, 0.37799043062200954, 0.37799043062200954, 0.37799043062200954, 0.37799043062200954, 0.3818615751789976, 0.3818615751789976, 0.3818615751789976, 0.3818615751789976, 0.3818615751789976, 0.3818615751789976, 0.3818615751789976, 0.3818615751789976, 0.38571428571428573, 0.38571428571428573, 0.38571428571428573, 0.38571428571428573, 0.38571428571428573, 0.38571428571428573, 0.38571428571428573, 0.38571428571428573, 0.38571428571428573, 0.38571428571428573, 0.38571428571428573, 0.38571428571428573, 0.38571428571428573, 0.3933649289099526, 0.3933649289099526, 0.3933649289099526, 0.3971631205673759, 0.4009433962264151, 0.41217798594847777, 0.41217798594847777, 0.41217798594847777, 0.41217798594847777, 0.41217798594847777, 0.41217798594847777, 0.41217798594847777, 0.41217798594847777, 0.41217798594847777, 0.41217798594847777, 0.4158878504672897, 0.4195804195804196, 0.4195804195804196, 0.4195804195804196, 0.4232558139534884, 0.4232558139534884, 0.4232558139534884, 0.4232558139534884, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4305555555555556, 0.4341801385681293, 0.4377880184331797, 0.4377880184331797, 0.4377880184331797, 0.4377880184331797, 0.4377880184331797, 0.4413793103448276, 0.4413793103448276, 0.44495412844036697, 0.44495412844036697, 0.44495412844036697, 0.44495412844036697, 0.44495412844036697, 0.44495412844036697, 0.44495412844036697, 0.44495412844036697, 0.44495412844036697, 0.44495412844036697, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.448512585812357, 0.4520547945205479, 0.4520547945205479, 0.45558086560364464, 0.46258503401360546, 0.46258503401360546, 0.4660633484162896, 0.4660633484162896, 0.4660633484162896, 0.4660633484162896, 0.4660633484162896, 0.4660633484162896, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.47297297297297297, 0.4764044943820225, 0.4764044943820225, 0.4764044943820225, 0.4764044943820225, 0.4764044943820225, 0.4764044943820225, 0.4764044943820225, 0.48322147651006714, 0.48322147651006714, 0.48997772828507796, 0.49333333333333335, 0.49333333333333335, 0.49333333333333335, 0.49333333333333335, 0.49333333333333335, 0.49333333333333335, 0.49333333333333335, 0.49667405764966743, 0.49667405764966743, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5033112582781457, 0.5033112582781457]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "heuristic_f1_scores = []\n",
    "unique_values = x_train['inter_api_access_duration(sec)'].sort_values().unique()  # Get sorted unique values\n",
    "\n",
    "# Loop through each possible threshold value\n",
    "for v in unique_values:\n",
    "    # Create heuristic predictions based on the threshold v\n",
    "    heuristic_pred = (x_test['inter_api_access_duration(sec)'] <= v).astype(int)  # 1 if condition is met, else 0\n",
    "    # Calculate the F1 score and append to the list\n",
    "    heuristic_f1_scores.append(f1_score(y_test, heuristic_pred))\n",
    "\n",
    "# Optionally print the results\n",
    "print(heuristic_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a line plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(unique_values, heuristic_f1_scores, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Inter API Access Duration (second)\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"F1 Score for Heuristic Rule\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "The heuristics based on the Inter API Access Duration (second) does not generate good F1-score. Hence, using heuristics as baseline is not applicable in this situation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "### Baseline\n",
    "The baseline is Random Forest Classifier. The reason I use Random Forest Classifier is because there are a lot of features (both numerical and categorical) and class imbalances in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random forest accuracy:  1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00       225\n",
      "        True       1.00      1.00      1.00       114\n",
      "\n",
      "    accuracy                           1.00       339\n",
      "   macro avg       1.00      1.00      1.00       339\n",
      "weighted avg       1.00      1.00      1.00       339\n",
      "\n",
      "Confusion Matrix:\n",
      " [[225   0]\n",
      " [  0 114]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Initialize and fit the Random Forest Classifier\n",
    "rforest=RandomForestClassifier(criterion='entropy', random_state=42)\n",
    "rforest.fit(x_train,y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = rforest.score(x_test, y_test)\n",
    "print('The random forest accuracy: ', accuracy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rforest.predict(x_test)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print('Classification Report:\\n', report)\n",
    "\n",
    "# Generate confusion matrix for better understanding of performance\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)  # TN FP (line 1), FN TP (line 2)\n",
    "print('Confusion Matrix:\\n', conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "The Random Forest Classifier demonstrate good performance with:\n",
    "- A high accuracy of 99.52%\n",
    "- Perfect precision and F1-scores for both classes\n",
    "- Very high recall for the False class and slightly less for the True class (99% recall)\n",
    "- The confusion matrix indicating that the model is mostly correct, with only two instance misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More advanced machine learning model\n",
    "I will use Histogram Gradient Boosting Classifier because this classifier uses boosting, where models are built sequentially, and each new model tries to correct the errors made by the previous ones. This often leads to better predictive performance, especially in challenging datasets. On the other hand, Random Forest Classifier uses bagging, where multiple trees are trained independently on random subsets of the data. This can lead to more stable predictions but might not capture complex relationships as effectively as boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"api_anomaly\")\n",
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/25 22:25:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-score: 1.0\n",
      "Confusion Matrix:\n",
      " [[225   0]\n",
      " [  0 114]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "with mlflow.start_run(run_name='GBT_baseline'):\n",
    "    # Define hyperparameters\n",
    "    params = {\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_iter\": 100,\n",
    "        \"max_leaf_nodes\": 31,\n",
    "        \"max_depth\": None,\n",
    "        \"l2_regularization\": 0,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    # Set MLflow tags and log hyperparameters\n",
    "    mlflow.set_tag(\"model_name\", \"HGBT\")\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    gbt = HistGradientBoostingClassifier(**params)\n",
    "    gbt.fit(x_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = gbt.predict(x_test)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # accuracy can also be calculated as: accuracy = gbt.score(x_test, y_test)\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Log metrics to MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Log the trained model\n",
    "    mlflow.sklearn.log_model(gbt, \"gbt_models\")\n",
    "\n",
    "# Output results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Without tuning, the Histogram Gradient Boosting Classifier achieved a slightly lower accuracy than the baseline Random Forest Classifier (99.29% vs. 99.52%)\n",
    "\n",
    "In the next part, we will tune the Histogram Gradient Boosting Classifier to identify if the Histogram Gradient Boosting Classifer can achieve better accuracy than the baseline Random Forest Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4672/3316417745.py:42: ExperimentalWarning: MLflowCallback is experimental (supported from v1.4.0). The interface can change in the future.\n",
      "  mlflc = MLflowCallback(\n",
      "[I 2024-12-25 22:27:58,006] A new study created in memory with name: no-name-6cea0314-694a-4dfe-a92b-52668859e94e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:27:58,374] Trial 0 finished with value: 0.6637168141592921 and parameters: {'learning_rate': 0.00011698169098103313, 'max_iter': 66, 'max_leaf_nodes': 14}. Best is trial 0 with value: 0.6637168141592921.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:27:58,821] Trial 1 finished with value: 0.6637168141592921 and parameters: {'learning_rate': 4.064089216297031e-05, 'max_iter': 80, 'max_leaf_nodes': 22}. Best is trial 0 with value: 0.6637168141592921.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:27:59,230] Trial 2 finished with value: 0.9941002949852508 and parameters: {'learning_rate': 0.00759053756189874, 'max_iter': 91, 'max_leaf_nodes': 16}. Best is trial 2 with value: 0.9941002949852508.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:27:59,679] Trial 3 finished with value: 0.6637168141592921 and parameters: {'learning_rate': 0.0012791257203521658, 'max_iter': 58, 'max_leaf_nodes': 21}. Best is trial 2 with value: 0.9941002949852508.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:00,127] Trial 4 finished with value: 1.0 and parameters: {'learning_rate': 0.02294131897623665, 'max_iter': 78, 'max_leaf_nodes': 13}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:00,566] Trial 5 finished with value: 0.9882005899705014 and parameters: {'learning_rate': 0.022013172571966742, 'max_iter': 58, 'max_leaf_nodes': 25}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:01,001] Trial 6 finished with value: 0.6637168141592921 and parameters: {'learning_rate': 7.477011145888668e-05, 'max_iter': 65, 'max_leaf_nodes': 23}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:01,466] Trial 7 finished with value: 0.9941002949852508 and parameters: {'learning_rate': 0.00675482969525411, 'max_iter': 62, 'max_leaf_nodes': 25}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:01,997] Trial 8 finished with value: 1.0 and parameters: {'learning_rate': 0.04103875704723304, 'max_iter': 83, 'max_leaf_nodes': 7}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:02,529] Trial 9 finished with value: 0.6637168141592921 and parameters: {'learning_rate': 0.0012597139034707604, 'max_iter': 70, 'max_leaf_nodes': 22}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:03,113] Trial 10 finished with value: 1.0 and parameters: {'learning_rate': 0.07317341295453406, 'max_iter': 96, 'max_leaf_nodes': 30}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:03,553] Trial 11 finished with value: 1.0 and parameters: {'learning_rate': 0.09190559810816178, 'max_iter': 82, 'max_leaf_nodes': 7}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:04,086] Trial 12 finished with value: 0.9882005899705014 and parameters: {'learning_rate': 0.013395681783075887, 'max_iter': 86, 'max_leaf_nodes': 7}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:04,594] Trial 13 finished with value: 0.6637168141592921 and parameters: {'learning_rate': 1.0039996277001805e-05, 'max_iter': 75, 'max_leaf_nodes': 11}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:05,135] Trial 14 finished with value: 1.0 and parameters: {'learning_rate': 0.03274885025462058, 'max_iter': 75, 'max_leaf_nodes': 11}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:05,666] Trial 15 finished with value: 0.6637168141592921 and parameters: {'learning_rate': 0.004061432968635906, 'max_iter': 50, 'max_leaf_nodes': 5}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:06,105] Trial 16 finished with value: 0.6637168141592921 and parameters: {'learning_rate': 0.0003479225818790039, 'max_iter': 99, 'max_leaf_nodes': 11}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:06,591] Trial 17 finished with value: 0.6637168141592921 and parameters: {'learning_rate': 0.0022770197028610466, 'max_iter': 88, 'max_leaf_nodes': 14}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:07,069] Trial 18 finished with value: 1.0 and parameters: {'learning_rate': 0.044680250453923924, 'max_iter': 82, 'max_leaf_nodes': 18}. Best is trial 4 with value: 1.0.\n",
      "/tmp/ipykernel_4672/3316417745.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
      "[I 2024-12-25 22:28:07,557] Trial 19 finished with value: 0.9882005899705014 and parameters: {'learning_rate': 0.01719605378068791, 'max_iter': 92, 'max_leaf_nodes': 9}. Best is trial 4 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'learning_rate': 0.02294131897623665, 'max_iter': 78, 'max_leaf_nodes': 13}\n",
      "Best trial accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def tune_hgbt(n_trials, mlflc, x_train, y_train, x_test, y_test):\n",
    "    # Define the objective function for Optuna\n",
    "    def objective(trial):\n",
    "        # Suggest hyperparameters\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 0.1),\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\", 50, 100),\n",
    "            \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 5, 30),\n",
    "        }\n",
    "\n",
    "        with mlflow.start_run(run_name='GBT_tuning'):\n",
    "            # Log hyperparameters\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            # Initialize and train the model\n",
    "            gbt = HistGradientBoostingClassifier(**params)\n",
    "            gbt.fit(x_train, y_train)\n",
    "\n",
    "            # Make predictions and calculate accuracy\n",
    "            y_pred = gbt.predict(x_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "            # Log metrics to MLflow\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "            return accuracy\n",
    "\n",
    "    # Create an Optuna study and optimize\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Return the best trial\n",
    "    return study.best_trial\n",
    "\n",
    "# Set Up MLflow Callback\n",
    "mlflc = MLflowCallback(\n",
    "    tracking_uri=\"sqlite:///mlflow.db\",\n",
    "    metric_name=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Call the tune_hgbt Function with x_test and y_test\n",
    "best_trial = tune_hgbt(20, mlflc, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# Step 5: Output the Best Hyperparameters\n",
    "print(\"Best trial parameters:\", best_trial.params)\n",
    "print(\"Best trial accuracy:\", best_trial.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train the Best Model with Optimal Hyperparameters\n",
    "best_gbt = HistGradientBoostingClassifier(**best_trial.params)\n",
    "best_gbt.fit(x_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = best_gbt.predict(x_test)\n",
    "\n",
    "# Step 8: Evaluate the Model Using Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Step 9: Output Evaluation Metrics\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor conclusion\n",
    "The best Histogram Gradient Boosting Classifier has a higher accuracy than the baseline Random Forest Classifier (99.76% vs. 99.52%)\n",
    "\n",
    "The parameters of the best Histogram Gradient Boosting Classifier are: learning_rate of 0.19, max_iter of 72, and max_leaf_nodes of 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation Feature Importances:\n",
      "      Feature  Importance       Std\n",
      "5  Feature 5    0.410226  0.021051\n",
      "2  Feature 2    0.042773  0.005887\n",
      "1  Feature 1    0.001475  0.001475\n",
      "0  Feature 0    0.000000  0.000000\n",
      "3  Feature 3    0.000000  0.000000\n",
      "4  Feature 4    0.000000  0.000000\n",
      "6  Feature 6    0.000000  0.000000\n",
      "7  Feature 7    0.000000  0.000000\n",
      "8  Feature 8    0.000000  0.000000\n",
      "9  Feature 9    0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance\n",
    "result = permutation_importance(gbt, x_test, y_test, n_repeats=30, random_state=42)\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_names = [f'Feature {i}' for i in range(x_train.shape[1])]  # Adjust this to your feature names\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': result.importances_mean,\n",
    "    'Std': result.importances_std\n",
    "})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Permutation Feature Importances:\\n\", importance_df)\n",
    "\n",
    "# Optional: Plot feature importances\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.barh(importance_df['Feature'], importance_df['Importance'], xerr=importance_df['Std'], color='skyblue')\n",
    "# plt.xlabel('Importance')\n",
    "# plt.title('Permutation Feature Importances')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- The above result indicates that the 6th feature (num_sessions) was the most important in predicting whether the API call was anomalous or not. Other features seemed unimportant.\n",
    "- The above result suggests that the model overfits and there seem to be a high correlation among the features in the final dataset I used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/c/Users/haanh/api-behavior-anomaly/model/hgbt_final.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(best_gbt, '/mnt/c/Users/haanh/api-behavior-anomaly/model/hgbt_final.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Based on accuracy, both the baseline Random Forest Classifier and the Histogram Gradient Boosting Classifier worked well (99.52% and 99.76%)\n",
    "- Based on feature importances, both models seemed to have overfitted and the features highly correlated.\n",
    "\n",
    "### Implication\n",
    "- Overfitting is highly expected here since the dataset was small (about 1700 instance) and I used 10 features, which can be too much for such a small dataset.\n",
    "- In the future, I am thinking about two ways to solve the problem of overfitting:\n",
    "+ Use the tabular data and graph data separately. More specifically, I will use either the tabular data or the graph data to model. \n",
    "+ Do feature engineering on the tabular data itself to identify highly correlated features and exclude them from the modelling stage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
